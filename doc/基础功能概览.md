# Paddle Serving 基础功能

## 概述
Paddle Serving 是一款成熟的模型部署服务，包括 C++ Serving、Pipeline Serving。对于部署 Paddle Serving 需要分为以下几个步骤：
- 获取Paddle Serving 部署所需的模型文件
- 启动 Paddle Serving
- 在 Client 中对常见模型数据进行处理
- 启动 Client，进行预测


因此，以下将从模型保存与转换、Serving 启动参数以及相关配置、常见模型数据处理和 Serving Client SDK 四方面进行介绍。


## 保存用于 Paddle Serving 部署的模型文件
Paddle Serving 提供接口：
- 从训练或预测脚本中保存模型。
- 将已有的静态图、动态图模型文件转换为 Paddle Serving 所需的模型文件。

详细，请参考[文档]()

## Serving 配置和启动参数

**一. C++ Serving**
Paddle Serving 提供 C++ Serving 用于高性能场景。文档将介绍 C++ Serving 启动命令中的相关参数以及对自定义配置文件进行详细描述。

详情，请参考[文档]()

**二. Pipeline Serving**
Paddle Serving 提供 Python Pipeline Serving 用于异构硬件、低精度推理、单算子多模型组合等场景。文档将详细介绍相关启动的参数和配置文件相关说明。

详情，请参考[文档]()

## 常见模型数据处理
Paddle Serving提供了非常灵活的pipeline web/rpc服务。因此需要一个统一的教程来指导在数据流的各个阶段，
我们的自然数据（文字/图片/稀疏参数表）会以何种形式存在并且传递。

详情，请参考[文档]()

## Serving Client SDK 介绍
对于支持的各种协议（BRPC、GRPC、HTTP1+Proto、HTTP1+Json）我们提供了部分的 Client SDK 示例供用户参考和使用。

**一. HTTP SDK**

使用 HTTP 协议发送请求更具有通用性，简单、直接、开发方便，但是在发送请求速率上一般。

详情，请参考[文档]()

**二. RPC SDK**

使用 RPC 发送效率更高，但是需要自定义协议，实现更为复杂。

详情，请参考[文档]()

