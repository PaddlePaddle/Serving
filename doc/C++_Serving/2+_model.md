# å¦‚ä½•ä½¿ç”¨C++å®šä¹‰æ¨¡å‹ç»„åˆ

å¦‚æœæ‚¨çš„æ¨¡å‹å¤„ç†è¿‡ç¨‹åŒ…å«2+çš„æ¨¡å‹æ¨ç†ç¯èŠ‚ï¼ˆä¾‹å¦‚OCRä¸€èˆ¬éœ€è¦det+recä¸¤ä¸ªç¯èŠ‚ï¼‰ï¼Œæ­¤æ—¶æœ‰ä¸¤ç§åšæ³•å¯ä»¥æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚

1. å¯åŠ¨ä¸¤ä¸ªServingæœåŠ¡ï¼ˆä¾‹å¦‚Serving-det, Serving-recï¼‰ï¼Œåœ¨æ‚¨çš„Clientä¸­ï¼Œè¯»å…¥æ•°æ®â€”â€”detå‰å¤„ç†â€”â€”è°ƒç”¨Serving-deté¢„æµ‹â€”â€”detåå¤„ç†â€”â€”recå‰å¤„ç†â€”â€”è°ƒç”¨Serving-recé¢„æµ‹â€”â€”recåå¤„ç†â€”â€”è¾“å‡ºç»“æœã€‚
    - ä¼˜ç‚¹ï¼šæ— é¡»æ”¹åŠ¨Paddle Servingä»£ç 
    - ç¼ºç‚¹ï¼šéœ€è¦ä¸¤æ¬¡è¯·æ±‚æœåŠ¡ï¼Œè¯·æ±‚æ•°æ®é‡è¶Šå¤§ï¼Œæ•ˆç‡ç¨å·®ã€‚
2. é€šè¿‡ä¿®æ”¹ä»£ç ï¼Œè‡ªå®šä¹‰æ¨¡å‹é¢„æµ‹è¡Œä¸ºï¼ˆè‡ªå®šä¹‰OPï¼‰ï¼Œè‡ªå®šä¹‰æœåŠ¡å¤„ç†çš„æµç¨‹ï¼ˆè‡ªå®šä¹‰DAGï¼‰ï¼Œå°†å¤šä¸ªæ¨¡å‹çš„ç»„åˆå¤„ç†è¿‡ç¨‹(ä¸Šè¿°çš„detå‰å¤„ç†â€”â€”è°ƒç”¨Serving-deté¢„æµ‹â€”â€”detåå¤„ç†â€”â€”recå‰å¤„ç†â€”â€”è°ƒç”¨Serving-recé¢„æµ‹â€”â€”recåå¤„ç†)é›†æˆåœ¨ä¸€ä¸ªServingæœåŠ¡ä¸­ã€‚æ­¤æ—¶ï¼Œåœ¨æ‚¨çš„Clientä¸­ï¼Œè¯»å…¥æ•°æ®â€”â€”è°ƒç”¨é›†æˆåçš„Servingâ€”â€”è¾“å‡ºç»“æœã€‚
    - ä¼˜ç‚¹ï¼šåªéœ€è¦ä¸€æ¬¡è¯·æ±‚æœåŠ¡ï¼Œæ•ˆç‡é«˜ã€‚
    - ç¼ºç‚¹ï¼šéœ€è¦æ”¹åŠ¨ä»£ç ï¼Œä¸”éœ€è¦é‡æ–°ç¼–è¯‘ã€‚

æœ¬æ–‡ä¸»è¦ä»‹ç»ç¬¬äºŒç§æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„åŸºæœ¬æ­¥éª¤å¦‚ä¸‹ï¼š
1. è‡ªå®šä¹‰OPï¼ˆå³å®šä¹‰å•ä¸ªæ¨¡å‹çš„å‰å¤„ç†-æ¨¡å‹é¢„æµ‹-æ¨¡å‹åå¤„ç†ï¼‰
2. ç¼–è¯‘
3. æœåŠ¡å¯åŠ¨ä¸è°ƒç”¨

# 1. è‡ªå®šä¹‰OP
ä¸€ä¸ªOPå®šä¹‰äº†å•ä¸ªæ¨¡å‹çš„å‰å¤„ç†-æ¨¡å‹é¢„æµ‹-æ¨¡å‹åå¤„ç†ï¼Œå®šä¹‰OPéœ€è¦ä»¥ä¸‹2æ­¥ï¼š
1. å®šä¹‰C++ .hå¤´æ–‡ä»¶
2. å®šä¹‰C++ .cppæºæ–‡ä»¶

## 1.1 å®šä¹‰C++ .hå¤´æ–‡ä»¶
å¤åˆ¶ğŸ‘‡çš„ä»£ç ï¼Œå°†å…¶ä¸­`/*è‡ªå®šä¹‰Classåç§°*/`æ›´æ¢ä¸ºè‡ªå®šä¹‰çš„ç±»åå³å¯ï¼Œå¦‚`GeneralDetectionOp`

æ”¾ç½®äº`core/general-server/op/`è·¯å¾„ä¸‹ï¼Œæ–‡ä»¶åè‡ªå®šä¹‰å³å¯ï¼Œå¦‚`general_detection_op.h`
``` C++
#pragma once
#include <string>
#include <vector>
#include "core/general-server/general_model_service.pb.h"
#include "core/general-server/op/general_infer_helper.h"
#include "paddle_inference_api.h"  // NOLINT

namespace baidu {
namespace paddle_serving {
namespace serving {

class /*è‡ªå®šä¹‰Classåç§°*/
    : public baidu::paddle_serving::predictor::OpWithChannel<GeneralBlob> {
 public:
  typedef std::vector<paddle::PaddleTensor> TensorVector;

  DECLARE_OP(/*è‡ªå®šä¹‰Classåç§°*/);

  int inference();
};

}  // namespace serving
}  // namespace paddle_serving
}  // namespace baidu
```
## 1.2 å®šä¹‰C++ .cppæºæ–‡ä»¶
å¤åˆ¶ğŸ‘‡çš„ä»£ç ï¼Œå°†å…¶ä¸­`/*è‡ªå®šä¹‰Classåç§°*/`æ›´æ¢ä¸ºè‡ªå®šä¹‰çš„ç±»åï¼Œå¦‚`GeneralDetectionOp`

å°†å‰å¤„ç†å’Œåå¤„ç†çš„ä»£ç æ·»åŠ åœ¨ğŸ‘‡çš„ä»£ç ä¸­æ³¨é‡Šçš„å‰å¤„ç†å’Œåå¤„ç†çš„ä½ç½®ã€‚

æ”¾ç½®äº`core/general-server/op/`è·¯å¾„ä¸‹ï¼Œæ–‡ä»¶åè‡ªå®šä¹‰å³å¯ï¼Œå¦‚`general_detection_op.cpp`

``` C++
#include "core/general-server/op/è‡ªå®šä¹‰çš„å¤´æ–‡ä»¶å"
#include <algorithm>
#include <iostream>
#include <memory>
#include <sstream>
#include "core/predictor/framework/infer.h"
#include "core/predictor/framework/memory.h"
#include "core/predictor/framework/resource.h"
#include "core/util/include/timer.h"

namespace baidu {
namespace paddle_serving {
namespace serving {

using baidu::paddle_serving::Timer;
using baidu::paddle_serving::predictor::MempoolWrapper;
using baidu::paddle_serving::predictor::general_model::Tensor;
using baidu::paddle_serving::predictor::general_model::Response;
using baidu::paddle_serving::predictor::general_model::Request;
using baidu::paddle_serving::predictor::InferManager;
using baidu::paddle_serving::predictor::PaddleGeneralModelConfig;

int /*è‡ªå®šä¹‰Classåç§°*/::inference() {
  //è·å–å‰ç½®OPèŠ‚ç‚¹
  const std::vector<std::string> pre_node_names = pre_names();
  if (pre_node_names.size() != 1) {
    LOG(ERROR) << "This op(" << op_name()
               << ") can only have one predecessor op, but received "
               << pre_node_names.size();
    return -1;
  }
  const std::string pre_name = pre_node_names[0];

  //å°†å‰ç½®OPçš„è¾“å‡ºï¼Œä½œä¸ºæœ¬OPçš„è¾“å…¥ã€‚
  GeneralBlob *input_blob = mutable_depend_argument<GeneralBlob>(pre_name);
  if (!input_blob) {
    LOG(ERROR) << "input_blob is nullptr,error";
    return -1;
  }
  TensorVector *in = &input_blob->tensor_vector;
  uint64_t log_id = input_blob->GetLogId();
  int batch_size = input_blob->_batch_size;

  //åˆå§‹åŒ–æœ¬OPçš„è¾“å‡ºã€‚
  GeneralBlob *output_blob = mutable_data<GeneralBlob>();
  output_blob->SetLogId(log_id);
  output_blob->_batch_size = batch_size;
  VLOG(2) << "(logid=" << log_id << ") infer batch size: " << batch_size;
  TensorVector *out = &output_blob->tensor_vector;

  //å‰å¤„ç†çš„ä»£ç æ·»åŠ åœ¨æ­¤å¤„ï¼Œå‰å¤„ç†ç›´æ¥ä¿®æ”¹ä¸Šæ–‡çš„TensorVector* in
  //æ³¨æ„iné‡Œé¢çš„æ•°æ®æ˜¯å‰ç½®èŠ‚ç‚¹çš„è¾“å‡ºç»è¿‡åå¤„ç†åçš„outä¸­çš„æ•°æ®

  Timer timeline;
  int64_t start = timeline.TimeStampUS();
  timeline.Start();
  // å°†å‰å¤„ç†åçš„inï¼Œåˆå§‹åŒ–çš„outä¼ å…¥ï¼Œè¿›è¡Œæ¨¡å‹é¢„æµ‹ï¼Œæ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¼šç›´æ¥ä¿®æ”¹outæŒ‡å‘çš„å†…å­˜ä¸­çš„æ•°æ®
  // å¦‚æœæ‚¨æƒ³å®šä¹‰ä¸€ä¸ªä¸éœ€è¦æ¨¡å‹è°ƒç”¨ï¼Œåªè¿›è¡Œæ•°æ®å¤„ç†çš„OPï¼Œåˆ é™¤ä¸‹é¢è¿™ä¸€éƒ¨åˆ†çš„ä»£ç å³å¯ã€‚
  if (InferManager::instance().infer(
          engine_name().c_str(), in, out, batch_size)) {
    LOG(ERROR) << "(logid=" << log_id
               << ") Failed do infer in fluid model: " << engine_name().c_str();
    return -1;
  }

  //åå¤„ç†çš„ä»£ç æ·»åŠ åœ¨æ­¤å¤„ï¼Œåå¤„ç†ç›´æ¥ä¿®æ”¹ä¸Šæ–‡çš„TensorVector* out
  //åå¤„ç†åçš„outä¼šè¢«ä¼ é€’ç»™åç»­çš„èŠ‚ç‚¹

  int64_t end = timeline.TimeStampUS();
  CopyBlobInfo(input_blob, output_blob);
  AddBlobInfo(output_blob, start);
  AddBlobInfo(output_blob, end);
  return 0;
}
DEFINE_OP(/*è‡ªå®šä¹‰Classåç§°*/);

}  // namespace serving
}  // namespace paddle_serving
}  // namespace baidu
```
### TensorVectoræ•°æ®ç»“æ„
TensorVector* inå’Œoutéƒ½æ˜¯ä¸€ä¸ªTensorVectorç±»å‹çš„æŒ‡æŒ‡é’ˆï¼Œå…¶ä½¿ç”¨æ–¹æ³•è·ŸPaddle C++APIä¸­çš„Tensorå‡ ä¹ä¸€æ ·ï¼Œç›¸å…³çš„æ•°æ®ç»“æ„å¦‚ä¸‹æ‰€ç¤º

``` C++
//TensorVector
typedef std::vector<paddle::PaddleTensor> TensorVector;

//paddle::PaddleTensor
struct PD_INFER_DECL PaddleTensor {
  PaddleTensor() = default;
  std::string name;  ///<  variable name.
  std::vector<int> shape;
  PaddleBuf data;  ///<  blob of data.
  PaddleDType dtype;
  std::vector<std::vector<size_t>> lod;  ///<  Tensor+LoD equals LoDTensor
};

//PaddleBuf
class PD_INFER_DECL PaddleBuf {
 public:

 explicit PaddleBuf(size_t length)
      : data_(new char[length]), length_(length), memory_owned_(true) {}

  PaddleBuf(void* data, size_t length)
      : data_(data), length_(length), memory_owned_{false} {}

  explicit PaddleBuf(const PaddleBuf& other);

  void Resize(size_t length);
  void Reset(void* data, size_t length);
  bool empty() const { return length_ == 0; }
  void* data() const { return data_; }
  size_t length() const { return length_; }
  ~PaddleBuf() { Free(); }
  PaddleBuf& operator=(const PaddleBuf&);
  PaddleBuf& operator=(PaddleBuf&&);
  PaddleBuf() = default;
  PaddleBuf(PaddleBuf&& other);
 private:
  void Free();
  void* data_{nullptr};  ///< pointer to the data memory.
  size_t length_{0};     ///< number of memory bytes.
  bool memory_owned_{true};
};
```

### TensorVectorä»£ç ç¤ºä¾‹
```C++
/*ä¾‹å¦‚ï¼Œä½ æƒ³è®¿é—®è¾“å…¥æ•°æ®ä¸­çš„ç¬¬1ä¸ªTensor*/
paddle::PaddleTensor& tensor_1 = in->at(0);
/*ä¾‹å¦‚ï¼Œä½ æƒ³ä¿®æ”¹è¾“å…¥æ•°æ®ä¸­çš„ç¬¬1ä¸ªTensorçš„åç§°*/
tensor_1.name = "new name";
/*ä¾‹å¦‚ï¼Œä½ æƒ³è·å–è¾“å…¥æ•°æ®ä¸­çš„ç¬¬1ä¸ªTensorçš„shapeä¿¡æ¯*/
std::vector<int> tensor_1_shape = tensor_1.shape;
/*ä¾‹å¦‚ï¼Œä½ æƒ³ä¿®æ”¹è¾“å…¥æ•°æ®ä¸­çš„ç¬¬1ä¸ªTensorä¸­çš„æ•°æ®*/
void* data_1 = tensor_1.data.data();
//åç»­ç›´æ¥ä¿®æ”¹data_1æŒ‡å‘çš„å†…å­˜å³å¯
//æ¯”å¦‚ï¼Œå½“æ‚¨çš„æ•°æ®æ˜¯intç±»å‹ï¼Œå°†void*è½¬æ¢ä¸ºint*è¿›è¡Œå¤„ç†å³å¯
```


# 2. ç¼–è¯‘
æ­¤æ—¶ï¼Œéœ€è¦æ‚¨é‡æ–°ç¼–è¯‘ç”Ÿæˆservingï¼Œå¹¶é€šè¿‡`export SERVING_BIN`è®¾ç½®ç¯å¢ƒå˜é‡æ¥æŒ‡å®šä½¿ç”¨æ‚¨ç¼–è¯‘ç”Ÿæˆçš„servingäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¹¶é€šè¿‡`pip3 install`çš„æ–¹å¼å®‰è£…ç›¸å…³pythonåŒ…ï¼Œç»†èŠ‚è¯·å‚è€ƒ[å¦‚ä½•ç¼–è¯‘Serving](../Compile_CN.md)

# 3. æœåŠ¡å¯åŠ¨ä¸è°ƒç”¨
## 3.1 Serverç«¯å¯åŠ¨

åœ¨å‰é¢ä¸¤ä¸ªå°èŠ‚å·¥ä½œåšå¥½çš„åŸºç¡€ä¸Šï¼Œä¸€ä¸ªæœåŠ¡å¯åŠ¨ä¸¤ä¸ªæ¨¡å‹ä¸²è”ï¼Œåªéœ€è¦åœ¨`--modelåä¾æ¬¡æŒ‰é¡ºåºä¼ å…¥æ¨¡å‹æ–‡ä»¶å¤¹çš„ç›¸å¯¹è·¯å¾„`ï¼Œä¸”éœ€è¦åœ¨`--opåä¾æ¬¡ä¼ å…¥è‡ªå®šä¹‰C++OPç±»åç§°`ï¼Œå…¶ä¸­--modelåé¢çš„æ¨¡å‹ä¸--opåé¢çš„ç±»åç§°çš„é¡ºåºéœ€è¦å¯¹åº”ï¼Œ`è¿™é‡Œå‡è®¾æˆ‘ä»¬å·²ç»å®šä¹‰å¥½äº†ä¸¤ä¸ªOPåˆ†åˆ«ä¸ºGeneralDetectionOpå’ŒGeneralRecOp`ï¼Œåˆ™è„šæœ¬ä»£ç å¦‚ä¸‹ï¼š
```python
#ä¸€ä¸ªæœåŠ¡å¯åŠ¨å¤šæ¨¡å‹ä¸²è”
python3 -m paddle_serving_server.serve --model ocr_det_model ocr_rec_model --op GeneralDetectionOp GeneralRecOp --port 9292
#å¤šæ¨¡å‹ä¸²è” ocr_det_modelå¯¹åº”GeneralDetectionOp  ocr_rec_modelå¯¹åº”GeneralRecOp
```

## 3.2 Clientç«¯è°ƒç”¨
æ­¤æ—¶ï¼ŒClientç«¯çš„è°ƒç”¨ï¼Œä¹Ÿéœ€è¦ä¼ å…¥ä¸¤ä¸ªClientç«¯çš„protoæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œä»¥OCRä¸ºä¾‹ï¼Œå¯ä»¥å‚è€ƒ[ocr_cpp_client.py](../../examples/C++/PaddleOCR/ocr/ocr_cpp_client.py)æ¥è‡ªè¡Œç¼–å†™æ‚¨çš„è„šæœ¬ï¼Œæ­¤æ—¶Clientè°ƒç”¨å¦‚ä¸‹ï¼š
```python
#ä¸€ä¸ªæœåŠ¡å¯åŠ¨å¤šæ¨¡å‹ä¸²è”
python3 è‡ªå®šä¹‰.py ocr_det_client ocr_rec_client
#ocr_det_clientä¸ºç¬¬ä¸€ä¸ªæ¨¡å‹çš„Clientç«¯protoæ–‡ä»¶å¤¹çš„ç›¸å¯¹è·¯å¾„
#ocr_rec_clientä¸ºç¬¬äºŒä¸ªæ¨¡å‹çš„Clientç«¯protoæ–‡ä»¶å¤¹çš„ç›¸å¯¹è·¯å¾„
```
æ­¤æ—¶ï¼Œå¯¹äºServerç«¯è€Œè¨€ï¼Œè¾“å…¥çš„æ•°æ®çš„æ ¼å¼ä¸`ç¬¬ä¸€ä¸ªæ¨¡å‹çš„Clientç«¯protoæ ¼å¼`å®šä¹‰çš„ä¸€è‡´ï¼Œè¾“å‡ºçš„æ•°æ®æ ¼å¼ä¸`æœ€åä¸€ä¸ªæ¨¡å‹çš„Clientç«¯proto`æ–‡ä»¶ä¸€è‡´ã€‚ä¸€èˆ¬æƒ…å†µä¸‹æ‚¨æ— é¡»å…³æ³¨æ­¤äº‹ï¼Œå½“æ‚¨éœ€è¦äº†è§£è¯¦ç»†çš„[protoçš„å®šä¹‰ï¼Œè¯·å‚è€ƒæ­¤å¤„](../Serving_Configure_CN.md)ã€‚
